{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "DM_Assignment_3.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/songseokbeom/ML_study/blob/master/Data_Mining/DM_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hzgrcukyjqqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPpjI0eGjqqr",
        "colab_type": "text"
      },
      "source": [
        "# Wine dataset 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvFp5cIijqqr",
        "colab_type": "code",
        "outputId": "aeeaa0ce-3080-4fa8-ea14-a62f0e5a7569",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "wine = load_wine()\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(wine.data, wine.target, random_state=777, test_size=0.2)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(142, 13)\n",
            "(36, 13)\n",
            "(142,)\n",
            "(36,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P10LSPKEjqqw",
        "colab_type": "text"
      },
      "source": [
        "# Wine dataset (참고용)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7UDEH9wyjqqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
        "sy = pd.Series(wine.target, dtype=\"category\")\n",
        "sy = sy.cat.rename_categories(wine.target_names)\n",
        "df['class'] = sy\n",
        "print(wine.DESCR)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IygpDF5Fjqqz",
        "colab_type": "text"
      },
      "source": [
        "# # Problem 1\n",
        "ID3 Decision Tree classifier.\n",
        "- decision-tree-id3 패키지를 pip install을 통해 설치하시오.\n",
        "- 위의 train 데이터(X_train, Y_train)를 이용하여 ID3 classifier 모델을 학습시키시오.\n",
        "- 위의 test 데이터(X_test, Y_test)를 이용해 정확도를 측정 후 출력하시오."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-TjSFZIjqqz",
        "colab_type": "code",
        "outputId": "8402a0d4-62d6-4cb6-bd64-83154b212d51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "##########Put your installation code here###############\n",
        "!pip install decision-tree-id3\n",
        "########################################################"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: decision-tree-id3 in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: nose>=1.1.2 in /usr/local/lib/python3.6/dist-packages (from decision-tree-id3) (1.3.7)\n",
            "Requirement already satisfied: scikit-learn>=0.17 in /usr/local/lib/python3.6/dist-packages (from decision-tree-id3) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from decision-tree-id3) (1.18.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.17->decision-tree-id3) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.17->decision-tree-id3) (0.14.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80UaceOvjqq2",
        "colab_type": "code",
        "outputId": "a684cbc6-e9cd-4b28-b212-7c63635bbc59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from id3 import Id3Estimator\n",
        "\n",
        "ID3_model = None\n",
        "accuracy = None\n",
        "############# Put your code here ################\n",
        "\n",
        "ID3_model = Id3Estimator()\n",
        "ID3_model.fit(X_train, Y_train)\n",
        "Y_hat = ID3_model.predict(X_test)\n",
        "accuracy = np.sum(np.where(Y_test - Y_hat==0, 1, 0))/X_test.shape[0]\n",
        "\n",
        "################################################# \n",
        "print('Test data prediction accuracy (ID3) : %f'%accuracy)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test data prediction accuracy (ID3) : 0.944444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdT_0Xyhjqq5",
        "colab_type": "text"
      },
      "source": [
        "# # Problem 2\n",
        "CART decision Tree classifier.\n",
        "- scikit-learn의 decision tree는 CART 기반 구현체이다.\n",
        "- Problem 1과 같은 방법으로 CART classifier 모델을 학습시키고 test데이터의 정확도를 출력하시오."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxGqA_x9jqq6",
        "colab_type": "code",
        "outputId": "3e859a97-9937-47c7-d626-5cfd9715b2a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "CART_model = None\n",
        "accuracy = None\n",
        "############# Put your code here ################\n",
        "CART_model = DecisionTreeClassifier()\n",
        "CART_model.fit(X_train, Y_train)\n",
        "Y_hat = CART_model.predict(X_test)\n",
        "accuracy = np.sum(np.where(Y_test - Y_hat==0, 1, 0)) / X_test.shape[0]           \n",
        "################################################# \n",
        "print('Test data prediction accuracy (CART) : %f'%accuracy)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test data prediction accuracy (CART) : 0.944444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WscZp2Sjqq8",
        "colab_type": "text"
      },
      "source": [
        "# # Problem 3\n",
        "Random Forest classifier.\n",
        "- scikit-learn의 random forest 패키지를 이용하여 모델을 학습시키고 test데이터의 정확도를 출력하시오."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejzMuTQQjqq9",
        "colab_type": "code",
        "outputId": "6cc83520-f668-4c07-c999-737942f610ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "Random_Forest_model = None\n",
        "accuracy = None\n",
        "############# Put your code here ################\n",
        "Random_Forest_model = RandomForestClassifier()\n",
        "Random_Forest_model.fit(X_train, Y_train)\n",
        "Y_hat = Random_Forest_model.predict(X_test)\n",
        "accuracy = np.sum(np.where(Y_hat - Y_test==0, 1, 0)) / X_test.shape[0]           \n",
        "################################################# \n",
        "print('Test data prediction accuracy (Random Forest) : %f'%accuracy)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 2 1 0 0 1 0 1 2 2 2 0 1 2 2 2 1 1 1 1 1 1 1 0 0 2 0 2 1 1 0 2 1 1 0]\n",
            "Test data prediction accuracy (Random Forest) : 1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dv7mxAUjqq_",
        "colab_type": "text"
      },
      "source": [
        "# # Problem 4\n",
        "XGBoost classifier.\n",
        "- XGBoost 패키지를 pip install을 통해 설치하시오.\n",
        "- 패키지를 불러와 모델을 학습시키고 test데이터의 정확도를 출력하시오."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YGGYH7SjqrA",
        "colab_type": "code",
        "outputId": "76b56503-5099-4a88-a033-1db9c6f8f2f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "##########Put your installation code here###############\n",
        "!pip install xgboost\n",
        "########################################################"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (0.90)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.18.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DcE-NwbjqrC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a06292d9-6a6a-4a2d-d373-a6e0de46d1cf"
      },
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "XGB_model = None\n",
        "accuracy = None\n",
        "############# Put your code here ################\n",
        "train_data = xgb.DMatrix(X_train, label=Y_train)\n",
        "test_data = xgb.DMatrix(X_test)\n",
        "param = {'objective': 'multi:softmax',\n",
        "         'num_class': 3,\n",
        "         'metric': 'multi_logloss'}\n",
        "XGB_model = xgb.train(param, train_data)\n",
        "Y_hat = XGB_model.predict(test_data)\n",
        "accuracy = np.sum(np.where(Y_hat - Y_test==0, 1, 0)) / X_test.shape[0]\n",
        "################################################# \n",
        "print('Test data prediction accuracy (XGBoost) : %f'%accuracy)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test data prediction accuracy (XGBoost) : 0.944444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adc8A1pHjqrE",
        "colab_type": "text"
      },
      "source": [
        "# # Problem 5\n",
        "LightGBM classifier.\n",
        "- LighGBM 패키지를 pip install을 통해 설치하시오.\n",
        "- 패키지를 불러와 모델을 학습시키고 test데이터의 정확도를 출력하시오."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl6d9pLHjqrE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "e2afe7c3-b96c-4497-e878-e02f9906372c"
      },
      "source": [
        "##########Put your installation code here###############\n",
        "!pip install lightgbm\n",
        "########################################################"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.6/dist-packages (2.2.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from lightgbm) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.18.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->lightgbm) (0.14.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXYjkUCNjqrI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dd119564-3a9d-446d-d48d-6599019d26f9"
      },
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "LGBM_model = None\n",
        "accuracy = None\n",
        "############# Put your code here ################\n",
        "train_data = lgb.Dataset(X_train, label=Y_train)\n",
        "param = {'objective': 'multiclass',\n",
        "         'num_class': 3,\n",
        "         'metric': 'multi_logloss'}\n",
        "LGBM_model = lgb.train(param, train_data)\n",
        "Y_hat = np.argmax(LGBM_model.predict(X_test), axis=1)\n",
        "accuracy = np.sum(np.where(Y_hat-Y_test==0, 1, 0)) / X_test.shape[0]\n",
        "################################################# \n",
        "print('Test data prediction accuracy (LightGBM) : %f'%accuracy)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test data prediction accuracy (LightGBM) : 0.944444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCcuZU1RjqrK",
        "colab_type": "text"
      },
      "source": [
        "# # Problem 6\n",
        "Naive Bayes classifier.\n",
        "- Scikit-learn의 Gaussian Naive Bayes classifier를 불러와 학습시키고, test데이터의 정확도를 출력하시오."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpPSLiRDjqrK",
        "colab_type": "code",
        "outputId": "e078a97a-7f88-4b82-df67-e09613b9a9a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "NB_model = None\n",
        "accuracy = None\n",
        "############# Put your code here ################\n",
        "NB_model = GaussianNB()\n",
        "NB_model.fit(X_train, Y_train)\n",
        "Y_hat = NB_model.predict(X_test)\n",
        "accuracy = np.sum(np.where(Y_hat - Y_test==0, 1, 0)) / X_test.shape[0]\n",
        "################################################# \n",
        "print('Test data prediction accuracy (NaiveBayes) : %f'%accuracy)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test data prediction accuracy (NaiveBayes) : 0.972222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNJ-LfMEjqrM",
        "colab_type": "text"
      },
      "source": [
        "# # Problem 7\n",
        "Ensemble classifier.\n",
        "- 위의 6개의 모델(ID3, CART, Random Forest, XGBoost, LightGBM, Naive Bayes)을 이용하여 Voting(6개의 예측 label중 가장 많이 예측된 label을 최종 prediction label로 택하는것)방법으로 최종 label을 택한 뒤, 예측 정확도를 출력하시오.\n",
        "- Scikit-learn의 VotingClassifier 패키지를 사용하여 작성.\n",
        "- 위에서 학습시킨 모델 변수명 사용 가능."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcYNgcGFjqrN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "094408fb-1f72-4aab-ff03-95ea16a533c5"
      },
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "ensemble_classifier = None\n",
        "accuracy = None\n",
        "############# Put your code here ################\n",
        "\n",
        "ensemble_classifier = VotingClassifier(estimators=[('ID3', ID3_model),\n",
        "                                                   ('CART', CART_model),\n",
        "                                                   ('Random Forest', Random_Forest_model),\n",
        "                                                   ('XGBoost', XGB_model),\n",
        "                                                   ('LightGBM', LGBM_model),\n",
        "                                                   ('Naive Bayes', NB_model)])\n",
        "ensemble_classifier.fit(X_train, Y_train)\n",
        "Y_hat = ensemble_classifier.predict(X_test)\n",
        "accuracy = np.sum(np.where(Y_hat - Y_test==0, 1, 0))\n",
        "################################################# \n",
        "print('Test data prediction accuracy (Ensemble) : %f'%accuracy)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-e75224319d96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                                                    \u001b[0;34m(\u001b[0m\u001b[0;34m'LightGBM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLGBM_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                                    ('Naive Bayes', NB_model)])\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mensemble_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mY_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensemble_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_hat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mtransformed_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mle_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;34m\"\"\"Get common fit operations.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_estimators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         if (self.weights is not None and\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_base.py\u001b[0m in \u001b[0;36m_validate_estimators\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m                 raise ValueError(\n\u001b[1;32m    248\u001b[0m                     \"The estimator {} should be a {}.\".format(\n\u001b[0;32m--> 249\u001b[0;31m                         \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_estimator_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m                     )\n\u001b[1;32m    251\u001b[0m                 )\n",
            "\u001b[0;31mValueError\u001b[0m: The estimator Id3Estimator should be a classifier."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jX4AEZrcgSV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}